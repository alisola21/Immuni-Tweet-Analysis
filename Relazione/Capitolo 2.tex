\chapter{Metodologia}
Di seguito, una breve panoramica delle tecnologie automatiche e semi-automatiche utilizzate per analizzare i tweet raccolti nelle fasi successive ed estrarre informazioni utili per lo scopo della nostra indagine.

\section{Analisi linguistica dei tweet}
Per analizzare i tweet raccolti dal punto di vista linguistico si utilizzerà la libreria \textbf{NLTK} (Natural Language ToolKit) per eseguire alcune operazioni linguistiche di base sui tweet come la \textbf{tokenizzazione}, ossia la divisione dei tweet in tokens le unità base dell'analisi linguistica e la \textbf{lemmatizzazione}, necessaria per trasformare ciascun token in un \textbf{lemma}, ossia la forma originaria che si trova come voce di un dizionario.
Su questi token verranno poi estratte alcune informazioni di base, come il numero complessivo, la lunghezza interna e la quantità di token presenti in ciascun tweet, ma anche informazioni più avanzate la loro distribuzione, individuando i token più frequenti.
%La PNL combina la linguistica computazionale - la modellazione del linguaggio umano basata su regole - con modelli statistici, di apprendimento automatico e di apprendimento profondo. Insieme, queste tecnologie consentono ai computer di elaborare il linguaggio umano sotto forma di testo o dati vocali e di "comprenderne" il significato completo, con l'intento e il sentimento di chi parla o scrive. che si occupa dello studio di una lingua dal punto di vista della "forma" tramite l'estrazione delle informazioni linguistiche necessarie per monitorare uno o più aspetti di una lingua a partire da un corpus annotato con diverse informazioni di natura linguistica.

%Ad oggi, il monitoraggio linguistico è diventato uno strumento molto importante e ampiamente utilizzato per lo studio delle caratteristiche di una lingua grazie all'implementazione di tecniche linguistico-computazionali di annotazione sempre più efficienti che operano sui vari livelli della struttura testuale.
%In particolare, oggi, grazie a queste tecniche di annotazione automatica del testo, è possibile studiare corpora sempre più grandi che in passato venivano annotati con tecniche manuali o semi-automatiche (\cite{montemagni_tecnologie_2013}) ed estrarre informazioni utili per analisi successive.
%Nello specifico, per analizzare il nostro corpus, composto dai tweet scritti sul tema App Immuni la cui costruzione è stata descritta nel capitolo precedente, si seguirà la cosiddetta \textit{Pipeline} di annotazione linguistica, ossia una catena di analisi formata da modelli di annotazione ordinati in senso crescente dove ad ogni livello di annotazione viene aggiunto un nuovo tipo di informazione testuale. \\

\section{Analisi dei sentimenti e dei temi:  \textit{Sentiment Analysis} e \textit{Topic Modeling} }
Dopo aver analizzato i tweet dal punto di vista strettamente linguistico, verranno poi eseguite due operazioni molto utili per estrarre informazioni riguardanti sia le opinioni, le emozioni e le credenze degli utenti che i temi più discussi, con l'utilizzo di due tecniche: la \textit{Sentiment analysis} e il \textit{Topic Modeling}

\subsection{Sentiment Analysis} \label{sec: sent_an}
Le \textbf{emozioni} sono un elemento molto importante nelle discussioni tra gli individui, soprattutto quelle in abito politico e nel marketing, in cui esse vengono utilizzate frequentemente nelle pubblicità per suscitare un interesse verso il prodotto da vendere. \\
Tuttavia, recentemente le emozioni hanno un ruolo sempre più importante anche nelle discussioni che avvengono sul web, in quanto gli utenti scrivono spesso post o commenti con un tono che può essere collegato ad un'emozione o ad una combinazione di emozioni.

Esistono quindi molte tecniche automatiche che permettono di estrarre le emozioni degli autori a partire da una risorsa testuale come la tecnica di Natual Language processing che prende il nome \textbf{Sentiment Analysis}.

Il \textit{Natural Language Processing - NLP} è una branchia dell'informatica, e più precisamente dell'intelligenza artificiale, che si occupa di dare ai computer la capacità di comprendere testi e parole pronunciate in modo simile a quello degli esseri umani, combinando la linguistica computazionale (ossia la modellazione del linguaggio tramite l'utilizzo di regole) con modelli statistici di apprendimento automatico, o \textit{Machine Learning} e apprendimento profondo, o \textit{Deep Learning} in modo tale da far comprendere al computer il linguaggio umano rendendolo così capace di comprendere testi, di emulare il linguaggio umano e addirittura comprendere il significato e il contesto di un particolare linguaggio scritto o orale.
Queste tecniche sono attualmente molto utilizzate in vari ambiti e sono alla base di molti sistemi di utilizzo quotidiano, come i traduttori automatici e gli assistenti vocali. 

La \textbf{Sentiment Analysis} è dunque una tecnica di Natural Language Processing (NLP), utilizzata per estrarre, a partire da grandi quantità di dati, il pensiero, l'atteggiamento, i punti di vista, le opinioni, le credenze, i commenti, le richieste, le domande e le preferenze espresse da un autore sulla base di un'emozione espressa sotto forma testuale, nei confronti di entità come servizi, questioni, individui, prodotti, eventi, argomenti e organizzazioni (\cite{lamba_sentiment_2022}).
Questa tecnica viene spesso utilizzata nell'ambito del web per rilevare le emozioni degli utenti sul web e di conseguenza comprendere la loro posizione riguardo ad un particolare argomento che ha creato dibattito tra gli utenti.

Infatti, in questa relazione, la suddetta tecnica verrà utilizzate per estrarre, a partire dai tweet scritti dagli utenti, le loro emozioni e nei confronti dell'app Immuni, cercando di capire quali siano le emozioni più diffuse e la loro tipologia, ossia se esiste una \textit{polarizzazione} degli utenti verso un sentimento negativo, positivo o neurale nei confronti dell'applicazione.
Per fare ciò utilizzeremo un particolare tipo di Sentiment Analysis, il modello \textbf{\textit{Feel-It}}\footnote{la cui documentazione è disponibile al seguente repository GitHub:\url{https://github.com/MilaNLProc/feel-it}} sviluppato nel 2021 dal MilanNLP Lab\footnote{https://milanlproc.github.io/}, il laboratorio di Natural Languge Processing dell'Università Bocconi di Milano, per eseguire la Sentiment Analysis su testi scritti in lingua italiana, il quale si basa sulla creazione di un corpus in italiano annotato secondo quattro emozioni: "\textit{gioia}", "\textit{paura}" "\textit{tristezza}" e "\textit{rabbia}" (\cite{bianchi_feel-it_2021}).
La scelta di questo modello è giustificata dal fatto che i tweet scritti sul tema App Immuni sono scritti in lingua italiana e l'utilizzo di questo modello avrebbe portato a risultati più accurati rispetto ad altri utilizzati invece per estrarre emozioni a partire da altre lingue, come quello elaborato per la lingua inglese da \cite{abdul-mageed-ungar-2017-emonet}.


 
\subsection{Topic Modeling} \label{sec:TopicModeling}
L'operazione di \textbf{Topic Modeling} o identificazione degli argomenti, è una tecnica di elaborazione automatica del testo impiegata per l'estrazione degli argomenti presenti all'interno di risorse testuali, raggruppandoli all'interno di \textit{clusters} o gruppi di argomenti.

Tuttavia, molte delle tecniche convenzionali, come la Latent Dirichlet Allocation (LDA) (Blei et al., 2003) non risultano ottimali per l'analisi linguistica perché non tengono conto un fattore molto importante, ossia il contesto in cui le parole sono inserite all'interno della stessa frase, descrivendo il documento come un insieme di parole scollegate tra di loro.
Per questo motivo, negli ultimi anni sono state sviluppate diverse tecniche di \textit{embedding} come il Bidirectional Encoder Representations from Transformers (BERT) (\cite{devlin-etal-2019-bert}) e le sue varianti (ad esempio, si veda quanto riportato in \cite{lee_bioinformatics_2019}), che hanno mostrato grandi risultati poiché codificano le parole e le frasi come vettori e li raggruppano in base alla loro similarità nello spazio vettoriale. Queste tecniche vengono sia utilizzate dai motori di ricerca, sia per estrarre e clusterizzare gli argomenti dei testi come dimostrato da \cite{sia-etal-2020-tired} secondo cui una parola può fare parte di un argomento o topic e quindi di un cluster se essa è molto frequente in qual particolare cluster.

Per condurre la nostra analisi verrà utilizzata una particolare tecnica di Topic Embedding derivata dall'orginale BERT, ossia il \textbf{BERTopic},  il quale estrae gli argomenti seguendo tre fasi.
Come prima cosa, ogni documento presente all'interno del corpus in analisi viene convertito in embedding, ossia in un vettore, utilizzando un modello linguistico pre-addestrato.
Successivamente vengono ridotte le dimensioni di questi embedding al fine di ottimizzare il processo di clusterizzazione. Infine, a partire da questi cluster vengono estratti gli argomenti più frequenti all'interno dei documenti e ordinati in ordine di frequenza all'interno del corpus.

Il BERTopic verrà utilizzato nell'ultima fase del lavoro per estrarre i temi più frequenti che ricorrono nei tweet scritti sul tema app Immuni, i quali verranno utilizzati per analizzare possibili correlazioni o gerarchizzazioni tra gli argomenti.

